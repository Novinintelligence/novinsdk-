import json
from datetime import datetime
from pathlib import Path

import numpy as np
import coremltools as ct
from coremltools.models.utils import save_spec

# Support both package and module execution
try:  # When executed as a package module
    from .neural_network import NeuralNetwork, ModelConfig  # type: ignore
except Exception:  # Fallback for direct script execution
    from novin_intelligence.neural_network import NeuralNetwork, ModelConfig  # type: ignore

# Paths & directories
BASE_DIR = Path(__file__).resolve().parent
ARTIFACTS_DIR = BASE_DIR / "artifacts"
ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)

# Try to locate the Swift package root to drop models into Sources/NovinIntelligence/Models
PROJECT_ROOT = None
for p in [BASE_DIR, *BASE_DIR.parents]:
    if (p / "Sources" / "NovinIntelligence").exists():
        PROJECT_ROOT = p
        break

if PROJECT_ROOT is not None:
    MODELS_DIR = PROJECT_ROOT / "Sources" / "NovinIntelligence" / "Models"
    MODELS_DIR.mkdir(parents=True, exist_ok=True)
else:
    MODELS_DIR = ARTIFACTS_DIR


def generate_swift_wrapper(destination: Path, input_size: int, output_size: int, model_class_name: str = "NovinAI") -> None:
    """Generate a Swift helper that wraps the Core ML model."""

    timestamp = datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
    swift_template = f"""//
//  {destination.name}
//  Auto-generated by convert_to_coreml.py
//
//  Generated on {timestamp}
//
//  Swift convenience wrapper for the NovinAI Core ML model.

import CoreML
import Foundation

public final class NovinAISecurityClassifier {{
    public static let shared = try? NovinAISecurityClassifier()

    private let model: {model_class_name}

    public init(configuration: MLModelConfiguration = MLModelConfiguration()) throws {{
        self.model = try {model_class_name}(configuration: configuration)
    }}

    @discardableResult
    public func prediction(features: [Float]) throws -> [Double] {{
        guard features.count == {input_size} else {{
            throw ModelLoadError.invalidInputSize(expected: {input_size}, actual: features.count)
        }}

        let mlArray = try MLMultiArray(shape: [NSNumber(value: {input_size})], dataType: .float32)
        for (index, element) in features.enumerated() {{
            mlArray[index] = NSNumber(value: element)
        }}

        let input = {model_class_name}Input(features: mlArray)
        let output = try model.prediction(input: input)
        let predictions = output.predictions

        return (0..<predictions.count).map {{ predictions[$0].doubleValue }}
    }}

    public enum ModelLoadError: Error, LocalizedError {{
        case invalidInputSize(expected: Int, actual: Int)

        public var errorDescription: String? {{
            switch self {{
            case let .invalidInputSize(expected, actual):
                return "Expected input size \\{expected} but received \\{actual}."
            }}
        }}
    }}
}}
"""

    destination.parent.mkdir(parents=True, exist_ok=True)
    destination.write_text(swift_template)


# Load config and network
config = ModelConfig()
nn = NeuralNetwork(config)

# Paths (adjust if needed)
model_path = BASE_DIR / "models" / "novin_ai_v2.0.json"
signature_path = Path(f"{model_path}.sig")
public_key_path = BASE_DIR / "models" / "model_public.pem"  # Assume exists or skip

# Load model (fallback to direct JSON parse if signature verify fails)
loaded = False
try:
    loaded = nn.load_model(model_path, signature_path, public_key_path)
    if loaded:
        print("Model loaded successfully from JSON with signature.")
except Exception as e:
    print(f"Signature-based load failed: {e}")

if not loaded and model_path.exists():
    try:
        payload = model_path.read_bytes()
        # Try to parse JSON directly
        try:
            model_dict = json.loads(payload.decode("utf-8"))
        except Exception:
            model_dict = json.loads((payload).decode("utf-8", errors="ignore"))
        nn._load_weights_from_dict(model_dict)
        nn.model_loaded = True
        print("Model weights loaded directly from JSON (no signature validation).")
    except Exception as e:
        print(f"Direct JSON load failed: {e}. Using initialized random weights.")

# Define layer architecture (matching your NN: 16384 -> 512 -> 256 -> 128 -> 4)
layer_sizes = [16384, 512, 256, 128, 4]
input_name = "features"
output_name = "predictions"

# Create builder
builder = ct.models.neural_network.NeuralNetworkBuilder(
    input_names=[input_name],
    output_names=[output_name],
    default_dtype=np.float32
)

# Get weights/biases from loaded NN
weights = nn.weights
biases = nn.biases

prev_layer = input_name
for i in range(len(layer_sizes) - 1):
    layer_name = f"linear_{i}"
    relu_name = f"relu_{i}" if i < len(layer_sizes) - 2 else None
    softmax_name = "softmax" if i == len(layer_sizes) - 2 else None

    # Weights and biases (fallback random if not loaded)
    w_shape = (layer_sizes[i], layer_sizes[i + 1])
    w = weights.get(f"layer_{i}", np.random.uniform(-0.1, 0.1, w_shape))
    b = biases.get(f"layer_{i}", np.zeros(layer_sizes[i + 1]))

    # Add inner product (linear layer)
    builder.add_inner_product_layer(
        name=layer_name,
        input_layers=[prev_layer],
        output_channels=layer_sizes[i + 1],
        W=w,
        b=b,
        has_bias=True
    )

    current_layer = layer_name

    # Add ReLU for hidden layers
    if relu_name:
        builder.add_activation_layer(
            name=relu_name,
            input_layers=[current_layer],
            ReLu_layer=True
        )
        current_layer = relu_name

    # Softmax for output layer
    if softmax_name:
        builder.add_softmax_layer(
            name=softmax_name,
            input_layers=[current_layer]
        )
        current_layer = softmax_name

    prev_layer = current_layer

# Finalize spec
spec = builder.spec

# Set input/output shapes (batch 1)
spec.description.input[0].shape = ct.Shape(shape=[1, layer_sizes[0]])
spec.description.output[0].shape = ct.Shape(shape=[1, layer_sizes[-1]])

# Quantize to 8-bit for mobile efficiency
quantized_spec = ct.models.neural_network.quantization.quantize_weights(spec, nbits=8)

# Create and save model artifacts (prioritize Models/ inside the Swift package if available)
mlmodel = ct.models.MLModel(quantized_spec)
coreml_model_path = MODELS_DIR / "NovinAI.mlmodel"
mlpackage_path = MODELS_DIR / "NovinAI.mlpackage"
swift_wrapper_path = MODELS_DIR / "NovinAISecurityClassifier.swift"

mlmodel.save(str(coreml_model_path))
save_spec(quantized_spec, str(mlpackage_path))
generate_swift_wrapper(swift_wrapper_path, layer_sizes[0], layer_sizes[-1])

print(f"Core ML model exported successfully to {coreml_model_path} (quantized 8-bit).")
print(f"Core ML package exported successfully to {mlpackage_path}.")
print(f"Swift wrapper generated at {swift_wrapper_path}.")
